# configuration for training NMT  (French and German to English)
# nohup sh run_trainer.sh >./train_logs/repeat_3_de_log_lstm.txt 2>&1 &
python -u trainer.py \
    --dynet-mem 3500 \
    --dynet-gpu \
    --input_dim 350 \
    --hidden_dim 350 \
    --model_type attention \
    --minibatch_size 32 \
    --percent_valid 500 \
    --reader_mode parallel \
    --train_src /data/rrjin/Graduation/data/bible-corpus/train_data/de_src.txt \
    --train_tgt /data/rrjin/Graduation/data/bible-corpus/train_data/de_tgt.txt \
    --trainer adam \
    --save bible_model_final_de_lstm \
    --load bible_model_final_de_lstm \
    --log_output bible_model_final_appended_de_lstm.log \
    --results_filename repeat_3_result \
    --directory_name final_tests_de_lstm


# configuration for extract language vector (French)
python -u trainer.py \
    --dynet-mem 3500 \
    --dynet-gpu \
    --input_dim 350 \
    --hidden_dim 350 \
    --model_type attention \
    --reader_mode parallel \
    --train_src /data/rrjin/Graduation/data/bible-corpus/train_data/fr_src.txt \
    --train_tgt /data/rrjin/Graduation/data/bible-corpus/train_data/fr_tgt.txt \
    --trainer adam \
    --load /data/rrjin/Graduation/bible_model_final_fr_lstm \
    --extract_lvs /data/rrjin/Graduation/data/language_vector \
    --word_list fr \
    --prefix_name fr__


# configuration for training NMT (French, German, Chinese, Japanese, Russian to English)
# nohup sh run_trainer.sh >./train_logs/combine_fr_de_zh_jap_ru_log_lstm.txt 2>&1 &
python -u trainer.py \
    --dynet-gpu \
    --input_dim 370 \
    --hidden_dim 370 \
    --model_type attention \
    --minibatch_size 32 \
    --percent_valid 800 \
    --reader_mode parallel \
    --train_src /data/rrjin/Graduation/data/bible-corpus/train_data/combine_fr_de_zh_jap_ru_src.txt \
    --train_tgt /data/rrjin/Graduation/data/bible-corpus/train_data/combine_fr_de_zh_jap_ru_tgt.txt \
    --trainer adam \
    --save bible_model_final_combine_fr_de_zh_jap_ru_lstm \
    --log_output bible_model_final_appended_combine_fr_de_zh_jap_ru_lstm.log \
    --results_filename result \
    --directory_name final_tests_combine_fr_de_zh_jap_ru_lstm

# configuration for training NMT (All 101 languages to English, using BPE algorithm)
# nohup sh run_trainer.sh >./train_logs/combined_all_data_bpe_log_lstm.txt 2>&1 &
python -u trainer.py \
    --dynet-gpu \
    --input_dim 512 \
    --hidden_dim 512 \
    --model_type attention \
    --minibatch_size 32 \
    --reader_mode parallel \
    --train_src /data/rrjin/Graduation/data/bible-corpus/train_data/train_combine_all_data_src_bpe.txt \
    --train_tgt /data/rrjin/Graduation/data/bible-corpus/train_data/train_combine_all_data_tgt_bpe.txt \
    --valid_src /data/rrjin/Graduation/data/bible-corpus/train_data/test_combine_all_data_src_bpe.txt \
    --valid_tgt /data/rrjin/Graduation/data/bible-corpus/train_data/test_combine_all_data_tgt_bpe.txt \
    --log_train_every_n 50000 \
    --trainer adam \
    --save bible_model_final_combine_all_data_bpe \
    --log_output bible_model_final_appended_combine_all_data_bpe.log \
    --results_filename result \
    --directory_name final_tests_combine_all_data_bpe